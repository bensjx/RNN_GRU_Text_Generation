{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWn904FCFz1j"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvNG3W_coq-S"
   },
   "outputs": [],
   "source": [
    "# deep learning library\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Input, Flatten, GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "from numpy.random import seed\n",
    "seed(4222)\n",
    "\n",
    "# general libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os, io\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdUEESz4GB3E"
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The data is subset into 1/4 of the original length to facilitate memory issues and speed up GRU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCQLr3L96h6v"
   },
   "outputs": [],
   "source": [
    "# Subset the data\n",
    "text = text[:int(len(text)/4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96aLXyEp5EKh"
   },
   "outputs": [],
   "source": [
    "# Split into sentences\n",
    "sents = text.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6B_hTOS6q02"
   },
   "outputs": [],
   "source": [
    "# Clean text data\n",
    "def clean_sentence(sentence):\n",
    "    text = sentence.replace(\"\\n\", \" \")  # Remove newline characters\n",
    "    text = \"\".join(v for v in text if v not in string.punctuation) # Remove punctuations\n",
    "    text = text.encode(\"utf8\").decode(\"ascii\",'ignore') # Convert utf8 to ascii\n",
    "\n",
    "    return text\n",
    "\n",
    "corpus = [clean_sentence(sent) for sent in sents] # Clean every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K93ctosM9bDV",
    "outputId": "bac8ce51-befd-4e85-bc76-1200f18c81e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[44, 82],\n",
       " [44, 82, 133],\n",
       " [44, 82, 133, 26],\n",
       " [44, 82, 133, 26, 598],\n",
       " [44, 82, 133, 26, 598, 178],\n",
       " [44, 82, 133, 26, 598, 178, 363],\n",
       " [44, 82, 133, 26, 598, 178, 363, 109],\n",
       " [44, 82, 133, 26, 598, 178, 363, 109, 20],\n",
       " [44, 82, 133, 26, 598, 178, 363, 109, 20, 105],\n",
       " [29, 105]]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding - convert from text to sequences (numbers)\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    '''\n",
    "    Convert each sentence to a list of ngram sequences\n",
    "    '''\n",
    "    # Tokenization\n",
    "    tokenizer.fit_on_texts(corpus) # Fit on our text sentences\n",
    "    total_words = len(tokenizer.word_index) + 1 # Total number of unique words in our vocabulary\n",
    "    \n",
    "    # Convert data to sequence of tokens \n",
    "    input_sequences = [] # House our final sequences\n",
    "    for line in corpus: # For every sentence\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0] # Convert a line of text to a line of sequence\n",
    "        for i in range(1, len(token_list)): # Generate ngrams\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10] # The first 10 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1dPI3AC9ctN"
   },
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    '''\n",
    "    Pad every sentence to the longest sentence in the corpus\n",
    "    '''\n",
    "    max_sequence_len = max([len(x) for x in input_sequences]) # Maximum length of sentence in corpus\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')) # Add paddings to before sentence\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1] # Set the last word as the label\n",
    "    label = ku.to_categorical(label, num_classes=total_words) # Convert to keras categorical variable\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGvAFHk_9dHP",
    "outputId": "0c2dfb8a-0a8c-4ea2-9e4a-334150258835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum sentence length is: 247\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 44]\n"
     ]
    }
   ],
   "source": [
    "print(\"The maximum sentence length is:\", max_sequence_len)\n",
    "print(predictors[0]) # Padded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-Vsyze_6mM4",
    "outputId": "dd45f5f0-a3fa-4905-8217-7adb693878e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 246, 10)           58430     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2460)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               1230500   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5843)              2927343   \n",
      "=================================================================\n",
      "Total params: 4,216,273\n",
      "Trainable params: 4,216,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "\n",
    "    # Initialise model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))\n",
    "    \n",
    "    # Hidden Layer - size 500, sigmoid activation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation = 'sigmoid'))\n",
    "    \n",
    "    # Output Layer - softmax activation\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    # Compile model - crossentropy loss\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1R_ZT2uA9zgG",
    "outputId": "83bb68b6-cd0c-40e8-b9d1-ec61064476f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 6.7493\n",
      "Epoch 2/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 6.4285\n",
      "Epoch 3/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 6.2325\n",
      "Epoch 4/20\n",
      "1505/1505 [==============================] - 8s 5ms/step - loss: 6.0335\n",
      "Epoch 5/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 5.8233\n",
      "Epoch 6/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 5.5786\n",
      "Epoch 7/20\n",
      "1505/1505 [==============================] - 8s 5ms/step - loss: 5.2942\n",
      "Epoch 8/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 4.9732\n",
      "Epoch 9/20\n",
      "1505/1505 [==============================] - 8s 5ms/step - loss: 4.6138\n",
      "Epoch 10/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 4.2436\n",
      "Epoch 11/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 3.9139\n",
      "Epoch 12/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 3.6340\n",
      "Epoch 13/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 3.3809\n",
      "Epoch 14/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 3.1474\n",
      "Epoch 15/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 2.9278\n",
      "Epoch 16/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 2.7208\n",
      "Epoch 17/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 2.5303\n",
      "Epoch 18/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 2.3516\n",
      "Epoch 19/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 2.1854\n",
      "Epoch 20/20\n",
      "1505/1505 [==============================] - 7s 5ms/step - loss: 2.0301\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oHMW546DLBa"
   },
   "outputs": [],
   "source": [
    "# Save weights so we do not have to retrain it\n",
    "model.save_weights(path + \"model1.h5\")\n",
    "\n",
    "# model.load_weights(path + \"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVUJ2eQpBxLb"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] # Tokenize seed text\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # Pad seed text\n",
    "        predicted = model.predict_classes(token_list, verbose=0) # Predict next word given seeded text\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items(): # Convert from sequence to string\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b66zW6HREHOM",
    "outputId": "5a6dd9c8-dbf3-4d83-a158-b1a0bac0beec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The crossentropy loss is: 2.030122995376587\n"
     ]
    }
   ],
   "source": [
    "# Obtain cross entropy loss\n",
    "loss_history = history.history[\"loss\"][-1]\n",
    "print(\"The crossentropy loss is:\", loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBPYBihuBxLu",
    "outputId": "8234d77a-8e27-47bc-a114-5e0d8f3c95c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thee I Would Be Reconciled To The Marketplace I Warrant To The Field Stir Did The\n",
      "Thus So I Am A Priest Of Mine Father The Duke Of Italy And Pray I\n",
      "Far And Bless His Grace And Hum And Shame To Undercrest My Elbow Persuading You All\n",
      "Cominius. I Would Be Reconciled To The Marketplace I Warrant You To Been Many To The\n",
      "Which I Would Be Consul And To Be A Perfecter That Being And Lose Me Betwixt\n",
      "Of You Have Been A Hundred Years And When You Please You To Be A Time\n",
      "Fear You To Be Revenged To Be His Perfecter Giber Than The Table Which Of These\n",
      "Their Hearts I Am Hushd Until You Know His General Purchasing Even Of Us For You\n",
      "Vassals, The Gods Have Rome And Harrow A Lamb And The Volscian Body Of Rome Deserve\n",
      "Him; So I Am Not I Would Be Sworn To Frame My Country I Be A\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    seed_word = random.choice(text.split())\n",
    "    print(generate_text(seed_word, 15, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRhpQFidtBlJ"
   },
   "source": [
    "#GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOm5pXintHNp",
    "outputId": "37f017a0-e1f5-4d92-95d3-45c04b1d9f55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 246, 10)           58430     \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 500)               768000    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5843)              2927343   \n",
      "=================================================================\n",
      "Total params: 3,753,773\n",
      "Trainable params: 3,753,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model2(max_sequence_len, total_words):\n",
    "\n",
    "    # Initialise model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len - 1))\n",
    "    \n",
    "    # Hidden Layer - GRU of size 500\n",
    "    model.add(GRU(500, activation = 'sigmoid'))\n",
    "    \n",
    "    # Output Layer - softmax activation\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    # Compile model - crossentropy loss\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model2 = create_model2(max_sequence_len, total_words)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYyOF-4XtHN8",
    "outputId": "f3b9c651-c9c9-4873-b995-060aa8144063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1505/1505 [==============================] - 652s 433ms/step - loss: 6.7365\n",
      "Epoch 2/20\n",
      "1505/1505 [==============================] - 658s 437ms/step - loss: 6.3899\n",
      "Epoch 3/20\n",
      "1505/1505 [==============================] - 656s 436ms/step - loss: 6.2130\n",
      "Epoch 4/20\n",
      "1505/1505 [==============================] - 648s 430ms/step - loss: 6.0484\n",
      "Epoch 5/20\n",
      "1505/1505 [==============================] - 644s 428ms/step - loss: 5.8756\n",
      "Epoch 6/20\n",
      "1505/1505 [==============================] - 642s 426ms/step - loss: 5.6803\n",
      "Epoch 7/20\n",
      "1505/1505 [==============================] - 642s 427ms/step - loss: 5.4720\n",
      "Epoch 8/20\n",
      "1505/1505 [==============================] - 642s 427ms/step - loss: 5.2524\n",
      "Epoch 9/20\n",
      "1505/1505 [==============================] - 644s 428ms/step - loss: 5.0085\n",
      "Epoch 10/20\n",
      "1505/1505 [==============================] - 642s 427ms/step - loss: 4.7300\n",
      "Epoch 11/20\n",
      "1505/1505 [==============================] - 641s 426ms/step - loss: 4.4269\n",
      "Epoch 12/20\n",
      "1505/1505 [==============================] - 639s 424ms/step - loss: 4.1214\n",
      "Epoch 13/20\n",
      "1505/1505 [==============================] - 643s 427ms/step - loss: 3.8513\n",
      "Epoch 14/20\n",
      "1505/1505 [==============================] - 642s 427ms/step - loss: 3.6245\n",
      "Epoch 15/20\n",
      "1505/1505 [==============================] - 655s 435ms/step - loss: 3.4289\n",
      "Epoch 16/20\n",
      "1505/1505 [==============================] - 663s 441ms/step - loss: 3.2543\n",
      "Epoch 17/20\n",
      "1505/1505 [==============================] - 659s 438ms/step - loss: 3.0977\n",
      "Epoch 18/20\n",
      "1505/1505 [==============================] - 668s 444ms/step - loss: 2.9513\n",
      "Epoch 19/20\n",
      "1505/1505 [==============================] - 664s 441ms/step - loss: 2.8152\n",
      "Epoch 20/20\n",
      "1505/1505 [==============================] - 657s 436ms/step - loss: 2.6871\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(predictors, label, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKeG-jKItHOA"
   },
   "outputs": [],
   "source": [
    "model2.save_weights(path + \"model2.h5\")\n",
    "\n",
    "# model.load_weights(path + \"model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoYzQp_jDX_F",
    "outputId": "0c2d8037-9a62-49f4-a196-974277e0840a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The crossentropy loss is: 2.687119722366333\n"
     ]
    }
   ],
   "source": [
    "# Obtain cross entropy loss\n",
    "loss_history2 = history2.history[\"loss\"][-1]\n",
    "print(\"The crossentropy loss is:\", loss_history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9vwnapl5J03",
    "outputId": "72ca8a81-bd31-4f57-a41b-d73255ea8c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thee O My Lord Of York And I Have Protector In The Tower Of The People\n",
      "Thus I Am Glad To Kill Him And A Death Of My Fortune And My Lord\n",
      "Far I Would Not Be Sworn For You Are Supper But He Is Durst Break His\n",
      "Cominius. I Am Glad To Strike A Ladyship That I Hope My Lord Young Edward But\n",
      "Which I Am Glad To Strike A Ladyship Judgment To Draw Him Only Duty With Him\n",
      "Of This Opinion Hath Been Outdares Thy Senseless Sword And His Brave Children Hath Owe His\n",
      "Fear My Lord Of York And I Have A Man If You Are A Man If\n",
      "Their Latest Refuge Keep My Part And Accept Me And I Shall Not See Your Life\n",
      "Vassals, A Verdict Noble Lord Hastings Even To Make A World To Conduct Us Be Changed\n",
      "Him; I Do Not Jest With Trial Rivers And I Will Not All All Names I\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random.seed(i)\n",
    "    seed_word = random.choice(text.split())\n",
    "    print(generate_text(seed_word, 15, model2, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj6j_ynLXvfa"
   },
   "source": [
    "## Simple RNN\n",
    "The crossentropy loss is 2.030122995376587\n",
    "\n",
    "The sentences produced are:\n",
    "1. Thee I Would Be Reconciled To The Marketplace I Warrant To The Field Stir Did The\n",
    "2. Thus So I Am A Priest Of Mine Father The Duke Of Italy And Pray I\n",
    "3. Far And Bless His Grace And Hum And Shame To Undercrest My Elbow Persuading You All\n",
    "4. Cominius. I Would Be Reconciled To The Marketplace I Warrant You To Been Many To The\n",
    "5. Which I Would Be Consul And To Be A Perfecter That Being And Lose Me Betwixt\n",
    "6. Of You Have Been A Hundred Years And When You Please You To Be A Time\n",
    "7. Fear You To Be Revenged To Be His Perfecter Giber Than The Table Which Of These\n",
    "8. Their Hearts I Am Hushd Until You Know His General Purchasing Even Of Us For You\n",
    "9. Vassals, The Gods Have Rome And Harrow A Lamb And The Volscian Body Of Rome Deserve\n",
    "10. Him; So I Am Not I Would Be Sworn To Frame My Country I Be A\n",
    "\n",
    "## GRU\n",
    "The crossentropy loss of the GRU is 2.687119722366333\n",
    "\n",
    "The sentences produced are:\n",
    "1. Thee O My Lord Of York And I Have Protector In The Tower Of The People\n",
    "2. Thus I Am Glad To Kill Him And A Death Of My Fortune And My Lord\n",
    "3. Far I Would Not Be Sworn For You Are Supper But He Is Durst Break His\n",
    "4. Cominius. I Am Glad To Strike A Ladyship That I Hope My Lord Young Edward But\n",
    "5. Which I Am Glad To Strike A Ladyship Judgment To Draw Him Only Duty With Him\n",
    "6. Of This Opinion Hath Been Outdares Thy Senseless Sword And His Brave Children Hath Owe His\n",
    "7. Fear My Lord Of York And I Have A Man If You Are A Man If\n",
    "8. Their Latest Refuge Keep My Part And Accept Me And I Shall Not See Your Life\n",
    "9. Vassals, A Verdict Noble Lord Hastings Even To Make A World To Conduct Us Be Changed\n",
    "10. Him; I Do Not Jest With Trial Rivers And I Will Not All All Names I\n",
    "\n",
    "\n",
    "Both the simple RNN and GRU produces readable sentences that does sound like English Literature but they do not make sense when read as a whole. GRU produced sentences which in my opinion, sounds more correct as compared to the simple RNN.\n",
    "\n",
    "However, it must be noted that I only trained for 20 epochs due to time and memory constraint. GRU took close to 4 hours to train while simple RNN only took a few minutes. Additionally, I only used 1/4 of the original corpus. If provided with the computational power, I would run it on the entire corpus for 200 iterations which should produce more readable sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mcIcWU2X6gg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A3_A0171412U.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
